---
title: "CRISP pre-processing and calculating allele frequencies"
name: Hamid Fotowatikha
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  html_notebook: default
---

```{r setup, include=FALSE}

# In the chuck below, we load in all the essential libraries for the data analysis
```{r Load Libraries, echo=FALSE, results=FALSE, include=FALSE, include=FALSE}
install.packages(c("BiocManager", "vcfR", "tidyr", "dplyr", "dbplyr", "patchwork", "tidyverse", "gsubfn", "purrr", 
                   "lme4", "lmerTest", "broom.mixed", "glmmTMB", "DHARMa", "car", "ggplot2", "qqman", "stats", 
                   "kableExtra", "knitr", "writexl", "furrr", "future", "future.apply", "data.table", "topr", "pheatmap"))

BiocManager::install(c("VariantAnnotation", "annotatr", "GenomicFeatures", "TxDb.Dmelanogaster.UCSC.dm6.ensGene", 
                        "org.Dm.eg.db", "AnnotationHub", "ensembldb", "clusterProfiler", "org.Hs.eg.db", "biomaRt", 
                        "Biostrings", "GenomicRanges", "BSgenome.Dmelanogaster.UCSC.dm6", "Rsamtools", "txdbmaker"))
#BiocManager
library(BiocManager)
#VariantAnnotation
library(VariantAnnotation)
# for vcf
library(vcfR)
# For parsing
library(tidyr)
library(dplyr)
library(dbplyr)
library(patchwork)
library(tidyverse)
library(gsubfn) 
library(purrr)
# for glm
library(lme4)
library(lmerTest) 
library(broom.mixed) # For tidying model output
library(glmmTMB) # for weighted GLMM
library(DHARMa) # to check statistical assumptions
library(car) # for type3 anova 
# for plots
library(ggplot2)
library(patchwork)  # Allows combining multiple ggplots
library(ggrepel) 
library(pheatmap) 
library(ggforce) 
library(RColorBrewer)
library(qqman) # for manhatten and qqplot
library(stats)       # KS test
# For tables:
library(kableExtra)
library(knitr)
# Some other packages for gene information extraction from Ensembl
library(BiocFileCache)
library(AnnotationHub)
library(ensembldb)  
# For annotation, gene name conversions (yes, we include human too)
#library(rPanglaoDB)
library(clusterProfiler)
library(org.Hs.eg.db)
library(org.Dm.eg.db)
library(biomaRt)
library(Biostrings) # to find reverse complement
# Save DEGs to excel sheet with gene information
library(writexl)
# For multithreading and memory management
library(furrr)
library(future)
library(future.apply)
library(data.table) # for large DFs and fast data manipulation
# For genetic annotation
library(txdbmaker)
library(annotatr)  # Main package for annotation
library(GenomicFeatures)  # Extracting genomic features from TxDb.
library(TxDb.Dmelanogaster.UCSC.dm6.ensGene)  # Transcript data for Drosophila DM6
library(org.Dm.eg.db)  # Gene annotations for Drosophila
library(GenomicRanges)  # Handling genomic intervals
# Exon mutation annotation
library(Rsamtools) # to import cutom .fa
library(VariantAnnotation)
library(BSgenome.Dmelanogaster.UCSC.dm6)
# TopR visualition
library(topr)
```

##### Description: #####
* Here we open the vcf output file generated by CRISP binary and we convert into correct format for further downstream analysis 
```{r, CRISP vcf processing, fig.height=4, fig.width=7, echo=FALSE, warning=FALSE, message=FALSE}
#################### Chunk #################### 
# // Open the CRISP vcf output file and  make data frame using the vcfR package
# // We extract the vcf values from CRISP INFO column as keys (e.g. "NP" "DP" "VT" "CT" "VP" "VF") in order to correctly parse for each variant
####################       #################### 

# Read the VCF file 
vcf_file <- "/Users/hamid/Desktop/GEN MSc Project/Data/NoFilteredReads_filtered_EMpass_output_SNVvariants_2L_X_variable.vcf"  #"/Users/hamid/Downloads/filtered_chromosomes_no_lowdepth.vcf"
vcf <- read.vcfR(vcf_file)
# Extract fixed fields (CHROM, POS, ID, REF, ALT, QUAL, FILTER, INFO)
fixed <- as.data.frame(vcf@fix)
# Extract all unique keys from the INFO column
all_keys <- unique(unlist(strsplit(paste(fixed$INFO, collapse = ";"), ";")))
all_keys <- unique(sapply(all_keys, function(x) strsplit(x, "=")[[1]][1]))

########## Function ########## 
# // This function extracts the INFO column into a named list for each row (variant)
# // The INFO column contains CRISP output of  "NP" "DP" "VT" "CT" "VP" "VF" for each variant.

# / NP: Number ofDP Pools With Data
# / DP: Total number of reads (+strand,-strand, overlapping paired-end reads) across all pools (reads with large gaps not included!)
# / VT: variant type, SNV | DELETION | INSERTION (only SNVs included)
# / CT: contingency table p-value for each variant allele
# / VP: Number of Pools with variant allele(s)
# / Ohter params include 10pb upstream and downstream of the variant and maximizations statisitcs.
parse_info <- function(info_string, all_keys) {
  # Initialize an empty list with all keys set to NA
  parsed_info <- setNames(rep(NA, length(all_keys)), all_keys)
  
  # Split the INFO string into key-value pairs
  info_list <- unlist(strsplit(info_string, ";"))
  for (kv in info_list) {
    key_value <- strsplit(kv, "=")[[1]]
    if (length(key_value) == 2) {
      parsed_info[key_value[1]] <- key_value[2]
    } else if (length(key_value) == 1) {
      parsed_info[key_value[1]] <- NA  # Keys without values
    }
  }
  return(parsed_info)
}
# Apply the parsing function to each row in the INFO column
info_parsed_list <- lapply(fixed$INFO, parse_info, all_keys = all_keys)


# Combine the parsed INFO fields into a single data frame
info_parsed_df <- bind_rows(info_parsed_list)
# Combine fixed fields (without INFO) and parsed INFO columns
vcf_cleaned <- cbind(fixed %>% dplyr::select(-INFO), info_parsed_df)



#################### Chunk #################### 
# // Use the new dataframe and append genottype information from the FORMAT info
# // vcf file must be from from previous step
# // Genotype information contains the number of forward and reverse reads supporiing the reference and alternative allele (including coverage)
####################       #################### 

# Extract genotype data (FORMAT fields for all samples)
genotype_data <- vcf@gt
# Combine fixed fields, parsed INFO fields, and sample-specific FORMAT columns
vcf_with_samples <- cbind(vcf_cleaned, genotype_data)
# Clean up column names for clarity
colnames(vcf_with_samples) <- gsub(" ", "_", colnames(vcf_with_samples))


#################### Chunk #################### 
# // Initial filtering based on SNPs
# // Deletions are generally hard to fund due to potenttial assmbly erros and mismaches witht the reference, so INDELS will be filtered out
####################       #################### 

# only keep SNVs, including  multi-alleleic SNVs
vcf_with_samples <- vcf_with_samples %>% filter(!grepl("SNV,", VT)) # multi allelic SNVs are notated as SNVs (while multi allelic SNV,DELETION,INDERTIONS exist)



#################### Chunk #################### 
# // Here we filter the columns that we no longer need
# // Additionally, we change the vcf file in such a way that multi-alleleic variants are split into multiple rows (one variant for each row)
# // Use the final dataframe and extract the: DP = Type=Integer,Description="Read Depth for each pool/sample (includes all reads), ADf = Type=Integer,Description="Number of reads aligned to the forward strand of the genome supporting reference allele and the alternate alleles in the order listed, ADr = Type=Integer,Description="Number of reads aligned to the reverse strand of the genome supporting reference allele and the alternate alleles in the order listed. ADb = Type=Integer,Description="Number of overlapping paired-end reads (read from both strands) supporting reference allele and the alternate alleles in the order listed
####################       #################### 

# Filtering useless columns
vcf_with_samples$ID <- NULL
vcf_with_samples$QUAL <- NULL
vcf_with_samples$HWEstats <- NULL
vcf_with_samples$MQS <- NULL
vcf_with_samples$HP <- NULL
vcf_with_samples$FORMAT <- NULL
# Remove the "Q30" charecter string form the colum nmaes of the samples
colnames(vcf_with_samples)[(ncol(vcf_with_samples) - 23):ncol(vcf_with_samples)] <- sub("^Q30", "", colnames(vcf_with_samples)[(ncol(vcf_with_samples) - 23):ncol(vcf_with_samples)])

# Identify rows with multiple ALT alleles
has_multiple_alts <- grepl(",", vcf_with_samples$ALT)
# Split the dataframe
single_alt_df <- vcf_with_samples[!has_multiple_alts, ]  # Rows with a single ALT
multiple_alt_df <- vcf_with_samples[has_multiple_alts, ]  # Rows with multiple ALTs
# Remove unneseary Fds before to save memory
rm(fixed)
rm(genotype_data)
rm(info_parsed_df)
rm(info_parsed_list)
rm(vcf)
rm(vcf_cleaned)
rm(all_keys)
rm(vcf_file)
rm(parse_info)
rm(vcf_with_samples)
gc()

# Save a subset of the multiple_alt_df to trace back variants that are multi allelic.
# This will be used for later steps
multiple_alt <- multiple_alt_df %>% dplyr::select(CHROM, POS, REF, ALT, VF)

########## Function ########## 
# // Function to process each row with multiple ALTs and correctly parse columns that must be split for each variant
# // We apply the function exclusively multiple_alt_df in order to save computation
# // The processed dataframe will be appended to then be appended to the full dataframe

# Define the processing function for rows with multiple ALTs
# Combine with rows that have a single ALT
process_row <- function(row) {
  # Split ALT into individual alleles
  alt_alleles <- unlist(strsplit(row["ALT"], ","))
  # Split relevant INFO columns into lists
  np_values <- unlist(strsplit(row["NP"], ","))
  dp_values <- unlist(row["DP"]) # For multi allelic SNPs, we display the total reads instead od per allele! (this parsing easier)
  vt_values <- unlist(row["VT"]) #, just like DP, rhe VT is shared for all multi allele samples, thus we dont split by comma
  ct_values <- unlist(strsplit(row["CT"], ","))
  vp_values <- unlist(strsplit(row["VP"], ","))
  vf_values <- unlist(strsplit(row["VF"], ","))
  ac_values <- unlist(strsplit(row["AC"], ","))
  af_values <- unlist(strsplit(row["AF"], ","))
  emstats_values <- unlist(strsplit(row["EMstats"], ","))
  # Split deduplicatedsorted (samples) columns into lists
  deduplicated_columns <- grep("deduplicatedsorted", names(row), value = TRUE)
  deduplicated_values <- lapply(row[deduplicated_columns], function(x) {
    # Split by ":" and further split dynamic components
    parts <- unlist(strsplit(x, ":"))
    list(
      AC = if (parts[1] == ".") parts[1] else unlist(strsplit(parts[1], ",")),  # Handle the AC per sample if starting with a dot "." 
      GQ = parts[2],
      DP = parts[3],
      ADf = unlist(strsplit(parts[4], ",")),
      ADr = unlist(strsplit(parts[5], ",")),
      ADb = unlist(strsplit(parts[6], ","))
    )
  })
  
  # Create a new dataframe for this INFO row
  new_rows <- data.frame(
    CHROM = rep(row["CHROM"], length(alt_alleles)),
    POS = rep(row["POS"], length(alt_alleles)),
    REF = rep(row["REF"], length(alt_alleles)),
    ALT = alt_alleles,
    FILTER = rep(row["FILTER"], length(alt_alleles)),
    NP = np_values,
    DP = dp_values,
    VT = vt_values,
    CT = ct_values,
    VP = vp_values,
    VF = vf_values,
    AC = ac_values,
    AF = af_values,
    EMstats = emstats_values,
    FLANKSEQ = rep(row["FLANKSEQ"], length(alt_alleles)),
    stringsAsFactors = FALSE
  )
  # Add deduplicatedsorted columns
  for (col in deduplicated_columns) {
    values <- deduplicated_values[[col]]
    new_rows[[col]] <- sapply(1:length(alt_alleles), function(i) {
      paste0(
        if (values$AC[1] == ".") { 
          values$AC 
          } else {
            paste0(values$AC[1], ",", values$AC[i + 1])
            }, ":", # As AC value per sample can lack values (specified as a simply dot "."), we make an if statement to iterate the vlaues if it exists
        values$GQ, ":",  # Static
        values$DP, ":",  # Static
        values$ADf[1], ",", values$ADf[i + 1], ":",  # First is shared, then specific
        values$ADr[1], ",", values$ADr[i + 1], ":",  # First is shared, then specific
        values$ADb[1], ",", values$ADb[i + 1])  # First is shared, then specific
    } )
    }
  return(new_rows)
  }
# Process the rows with multi allelic samples and call the process_row function
expanded_multiple_alt_df <- do.call(rbind, apply(multiple_alt_df, 1, process_row))
# Combine the processed rows with the original single ALT rows
vcf_with_samples <- rbind(single_alt_df, expanded_multiple_alt_df) # here we save computation as we only processed the required rows and recombine with the original!

# Remove dataframes, funtions and vectors and clean memory
rm(fixed)
rm(genotype_data)
rm(info_parsed_df)
rm(info_parsed_list)
rm(vcf)
rm(vcf_cleaned)
rm(all_keys)
rm(vcf_file)
rm(parse_info)
rm(expanded_multiple_alt_df)
rm(multiple_alt_df)
rm(single_alt_df)
rm(has_multiple_alts)
rm(process_row)
gc()


#################### Chunk #################### 
# // Filter again on EMfaill (cariants that failed Expectation–maximization algorithm), as these were not filtered before for multi allelic SNPs, as these were nested in the vcf file
# // Thus some multi allelic SNPs will be removed for some samples (~5000 SNPs)
####################       ####################

# Remove all variant that have an EMfail (failed statisitcs)
vcf_with_samples <- vcf_with_samples %>% filter(!grepl("EMfail", VF))



#################### Chunk ####################
# Filtering based on CRISP CT pvalue
# // Initial CT vlaues are 10log(pvalue), thus we take 10^pvalue to calculate thte true pvalue and replace the CT column with that
# // By filtering on CT-pvalue we follow CRISPs methodology and loose (multi- and single allelic) variants based on sequencing bias. If they have good coverage or seem too good while pvalue >0.05, then these have allele frequencies of nearly 1 or 0 accords all pools, so useless for the GLMM anyway. In other words, we include is polymorphic sites and remove monomorphic site in most cases
# // In addition by filtering on CT pvalue, we almost excluded all sites with a coverage below 30 for each sample
# // It must also be noted that in CRISPs DP and AF values are calculated based in filtered reads only, thus read with large gaps are not included in the CT pvalue calculation. By filtering based on pvalue, we also exclude most alleles that are from duplicated genomic region (CNV) and Reads from repetitive sequences falsely pile up in certain regions. As inconsistent mapping of the same reads across 24 samples will result in failed contingency table approach in CRISP and result in large CT pvalues.
####################       #################### 

# calculate pvalues from CT and replace CP column with that
vcf_with_samples$CT <- as.numeric(vcf_with_samples$CT) # change character to numeric
vcf_with_samples$CT <- 10^vcf_with_samples$CT
# Now filter based on CT 0.05
vcf_with_samples <- subset(vcf_with_samples, CT <= 0.00032) # 0.00032 Corresponds to -3.5 CRISP default



#################### Chunk #################### 
# // Instead of calling SNPs per sample, CRISP considers allele counts across all samples in their contingency table approach (including variable poolsize information), which already eliminated the odds of missing variants that may be low-frequency in individual samples but still true SNPs.
# // To further collaborate on this and minimize false-positive rare variants, we will considered only sites with a minor allele count of at least 48 as being polymorphic across all samples (pools), meaning a combines 48 reads across all samples must support the minor allele. While this method dos not considered the variable poolszie for each sample, assuming uniform sequencing depth, if an rare allele is present in for example the CE samples (relative to CL), then they must contain 48/12(samples) = 4 reads per sample to be considered an true minor allele instead of sequencing error. Another example: If each pool contains 50 individuals (100 because heterogeneous), and if a SNP is truly segregating, it should appear at relatively high counts. With 24 pools (12 CE, 12 CL), if a variant is present in 5% of individuals in one pool, it contributes ~5 reads (assuming uniform sequencing depth of 100x), so a total of 24*5 = 120 reads for all samples, or half of the samples (12 CE or CL) if it is truly unique in one of the two lines. 120 is larger than our threshold of 48, and minimizes the impact of sequencing errors; it assumes that alleles with a high allele count in one or few populations, or alleles that have a low number in multiple populations, are correctly called SNPs. 
# // Likewise, If the depth is 30 per samples, then we must have at least ~2 read supporting the minor allele across all samples or 4 for only the CE or CL line. 
# // We consider a MAC (Compute Minor Allele Count) of 48 as conservative but still allows detection of variants at very low frequencies across the dataset. On top of that, a MAC of 48 will further remove low-coverage variants across all samples (pools)
# // Keep in mind that this is an additional filtering step on top of CRISPs excellent variant detection based on their contingency table and Expectation–maximization approach with a false discovery rate of 3–5%. This will minimize the false-positives, and with our penalized GLMM method, we even include a strict filter one small (but otherwise significant) differences between CE and CL lines
# // We further remove all variants with a <360 read across all pools (supports ~30 reads for at least 12 samples). While CRISP is already robust against <30 reads per sample due to its contingency table approach, this additional filtering will be slighly more conservative
####################       #################### 

# Minor allele read count of 48
# Ensure DP column is properly summed before calculations
vcf_with_samples$DP <- sapply(strsplit(as.character(vcf_with_samples$DP), ","), function(x) sum(as.numeric(x))) # and make numeric
# Calculate MAC across all samples
# Compute ALT count
vcf_with_samples$AF <- as.numeric(vcf_with_samples$AF) 
vcf_with_samples$ALT_count <- vcf_with_samples$AF * vcf_with_samples$DP
# Compute REF count
vcf_with_samples$REF_count <- (1 - vcf_with_samples$AF) * vcf_with_samples$DP # already nummeric from function above
# Compute Minor Allele Count
vcf_with_samples$MAC <- pmin(vcf_with_samples$ALT_count, vcf_with_samples$REF_count) # with pmin we take the minor allele
# Filter for MAC ≥ 48
vcf_with_samples <- subset(vcf_with_samples, MAC >= 48)

# Remove variants with <360 reads across all samples (supports 30 reads for at least 12 samples)
vcf_with_samples <- subset(vcf_with_samples, DP >= 360) 

# Filtering useless columns
vcf_with_samples$MAC <- NULL
vcf_with_samples$ALT_count <- NULL
vcf_with_samples$REF_count <- NULL



#################### Chunk #################### 
# // Use the final dataframe and extract the: DP = Type=Integer,Description="Read Depth for each pool/sample (includes all reads), ADf = Type=Integer,Description="Number of reads aligned to the forward strand of the genome supporting reference allele and the alternate alleles in the order listed, ADr = Type=Integer,Description="Number of reads aligned to the reverse strand of the genome supporting reference allele and the alternate alleles in the order listed. ADb = Type=Integer,Description="Number of overlapping paired-end reads (read from both strands) supporting reference allele and the alternate alleles in the order listed
# // Allele frequencies per sample are calculated by leveraging the params above which are nested (e.g ADr = xx,yy where xx represents the reference and yy the alternate).
####################       ####################

########## Function ########## 
# // Function to calculate the allele frequency from ADf, ADr, ADb and DP for each sample

calculate_af_multiallelic <- function(format_string) {
  # Skip NA or malformed FORMAT strings
  if (is.na(format_string) || format_string == ".") return(NA)
  # Split the FORMAT string into components
  format_fields <- unlist(strsplit(format_string, ":"))
  # Extract DP (Total Read Depth)
  DP <- as.numeric(format_fields[3])
  # Extract ADf, ADr, and ADb, and parse them into separate vectors
  ADf <- as.numeric(unlist(strsplit(format_fields[4], ",")))  # ADf values
  ADr <- as.numeric(unlist(strsplit(format_fields[5], ",")))  # ADr values
  ADb <- as.numeric(unlist(strsplit(format_fields[6], ",")))  # ADb values
  # Ensure all vectors are of the same length
  max_alleles <- max(length(ADf), length(ADr), length(ADb))
  ADf <- c(ADf, rep(0, max_alleles - length(ADf)))
  ADr <- c(ADr, rep(0, max_alleles - length(ADr)))
  ADb <- c(ADb, rep(0, max_alleles - length(ADb)))
  # Calculate total depth for each alternate allele
  AD_ALT <- ADf[-1] + ADr[-1] + ADb[-1]  # Skip the first value (reference allele)
  # Calculate allele frequencies for each alternate allele
  if (!is.na(DP) && DP > 0) {
    AFs <- AD_ALT / DP
    return(AFs)  # Return a vector of allele frequencies
  } else {
    return(rep(NA, max_alleles - 1))  # Return NAs if DP is invalid
  }
}

# List of sample columns (FORMAT fields for each sample)
sample_columns <- colnames(vcf_with_samples)[grepl("deduplicated", colnames(vcf_with_samples))]
# Calculate allele frequencies for all alternate alleles
max_alternate_alleles <- 0
for (sample in sample_columns) {
  # Apply the function to calculate AFs for each alternate allele per sample
  afs <- lapply(vcf_with_samples[[sample]], calculate_af_multiallelic)
  # Find the maximum number of alternate alleles across all variants for this sample
  max_alleles <- max(sapply(afs, length))
  max_alternate_alleles <- max(max_alternate_alleles, max_alleles) # OLD CODE, we already split the multi alleles in rows, so this is useless!
  # Expand AF vectors to have a consistent length (fill with NAs)
  afs <- lapply(afs, function(x) c(x, rep(NA, max_alleles - length(x))))
  # Add new columns for each alternate allele's AF
  for (i in seq_len(max_alleles)) {
    column_name <- paste0(sample, "_AF", i)
    vcf_with_samples[[column_name]] <- sapply(afs, function(x) x[i])
  }
}
rm(afs) # Remove temp lists

# Save the updated table to a file
#write.table(vcf_with_samples, "vcf_with_sample_allele_frequencies_dynamic.tsv", sep = "\t", row.names = FALSE, quote = FALSE)

# Summary: Maximum number of alternate alleles handled
cat("Maximum number of alternate alleles across all variants:", max_alternate_alleles, "\n")



#################### Chunk #################### 
# // Prepare the dataframe for the GLM model 
# // Including the alternative (multi) alleles (not included in previous version of code!)
# // We also define the metadata structure for the GLM model
####################       ####################

# Identify raw sample columns (excluding derived columns like _AF_ALT)
raw_sample_columns <- colnames(vcf_with_samples)[grepl("^deduplicatedsorted[^_]+$", colnames(vcf_with_samples))]
raw_sample_columns
# Identify derived allele frequency columns (We dont use _AF_ALT1, _AF_ALT2 anymore as each allele, including multi-alleles have their own rows)
af_columns <- colnames(vcf_with_samples)[grepl("_AF", colnames(vcf_with_samples))]
af_columns

# Pivot raw sample columns to long format
raw_long <- vcf_with_samples %>%
  pivot_longer(cols = all_of(raw_sample_columns), names_to = "sample",values_to = "raw_sample_data")
# shorter version
af_long <- vcf_with_samples %>%
  pivot_longer(cols = all_of(af_columns), names_to = "af_column", values_to = "allele_frequency") %>%
  mutate(alt_allele = as.numeric(sub(".*_AF", "", af_column))) %>% # Extract the alternate allele number from column names
  dplyr::select(CHROM, POS, REF, ALT, FILTER, NP, af_column, DP, VT, CT, VP, VF, AC, AF, FLANKSEQ, EMstats, allele_frequency) 
# add coluns from raw_long to af_long
af_long$sample <- raw_long$sample
# add sample column from raw_long to af_long
af_long$sample <- raw_long$sample

# Remove large dataframes and clear memory:
rm(vcf_with_samples)
rm(raw_long)
rm(af_columns)
rm(column_name)
rm(i)
rm(max_alleles)
rm(max_alternate_alleles)
rm(raw_sample_columns)
rm(sample)
rm(sample_columns)
rm(calculate_af_multiallelic)
gc()

# Define the sample metadata ad structure for the stasitical analysis 
sample_metadata <- data.frame(
  sample = c(
    "deduplicatedsortedB2", "deduplicatedsortedF18", "deduplicatedsortedD34", "deduplicatedsortedD10", "deduplicatedsortedB26", "deduplicatedsortedF42",
    "deduplicatedsortedC3", "deduplicatedsortedA19", "deduplicatedsortedE35", "deduplicatedsortedC27", "deduplicatedsortedE11", "deduplicatedsortedA43",
    "deduplicatedsortedF6", "deduplicatedsortedD22", "deduplicatedsortedB38", "deduplicatedsortedB14", "deduplicatedsortedF30", "deduplicatedsortedD46",
    "deduplicatedsortedA7", "deduplicatedsortedE23", "deduplicatedsortedC39", "deduplicatedsortedC15", "deduplicatedsortedA31", "deduplicatedsortedE47"
  ),
  biological_group = c(rep("CE2", 6), rep("CE3", 6), rep("CL2", 6), rep("CL3", 6)),
  age_group = rep(c(rep("Y", 3), rep("O", 3)), 4),  # 3 Young, 3 Old per group
  fly_group = c(rep("short_living", 12), rep("long_living", 12))  # CE = short, CL = long
)

# Add metadata (biological group, age group)
vcf_short <- af_long %>% left_join(sample_metadata, by = "sample")

# Remove old DFs and clean memory
rm(af_long)
rm(sample_metadata)
gc()



#################### Chunk #################### 
# // Here we adjust/manipulate allele frequencies so that absolute 0 and 1s approximate those numbers
# // We do so to make it compatible with the Beta distribution
####################       ####################

# Shift 0s to a small value and 1s to just below 1 to make it compatible with the GLMM model and Beta distribution
# .Machine$double.eps is the smallest number so that 1 + eps > 1 in double-precision floars in R
# - If allele frequency is exactly 0, replace it with epsilon (smallest positive distinguishable number)
# - If allele frequency is exactly 1, replace it with (1 - epsilon) to avoid numerical issues
# - All other values remain unchanged in allele_frequencies
epsilon <- .Machine$double.eps
vcf_short$allele_frequency <- ifelse(
  vcf_short$allele_frequency == 0, epsilon,
  ifelse(vcf_short$allele_frequency == 1, 1-epsilon, vcf_short$allele_frequency)
)
rm(epsilon)

# .Machine$double.eps is the smallest number so that 1 + eps > 1 in double-precision floars in R
# We use this to ensures that values are slightly adjusted without introducing a significant bias
#epsilon <- .Machine$double.eps
# Shift 0s to a small value and 1s to just below 1 to make it compatible with the GLMM model and Beta distribution
# - If allele frequency is exactly 0, replace it with epsilon (smallest positive distinguishable number)
# - If allele frequency is exactly 1, replace it with (1 - epsilon) to avoid numerical issues
# - All other values remain unchanged in allele_frequencies
#vcf_short$allele_frequency <- pmax(epsilon, pmin(1 - epsilon, vcf_short$allele_frequency))
#rm(epsilon)



#################### Chunk #################### 
# // Make new variant ID based on chromosome, location, alternate allele.
# // This helps to ensure that multi-alleic variants can be parsed separately if needed (even when on they are located in same CHROM and POS)
# // This also allows to calculate the weights per variant_id
# // Also add technical replicates column
####################       ####################

# remove unneeded columns
vcf_short$X <- NULL
vcf_short$af_column <- NULL

# Make new variant ID based on chromosome, location, alternate allele.
# This helps to ensure that multi-alleic variants can be parsed separetly if needed (even when on the )
vcf_short <- vcf_short %>%
  mutate(variant_id = paste(CHROM, POS, ALT, sep = "_")) %>% # take CHROM, POS, and ALT to make all unque variants
  relocate(variant_id, .before = 5)  # Move variant_id to the 5th column, after ALT col

# Make the technical replicates column (e.g. CE2_1, CE2_2, CE2_3, ect...)
vcf_short <- vcf_short %>%
  group_by(biological_group, variant_id) %>%
  mutate(technical_group = paste0(biological_group, "_", row_number())) %>%
  ungroup()



#################### Chunk #################### 
# // Calculate the penalty values (weight) for each variant_id and store in vcf_short
# // The formula equals: (allele_diff^2) * (1 - min_frequency)
# // Thus we heavily penalize small differences in average allele frequency differences between CE an CL
# // Additional penalty for large allele frequencies to account for allele fixation
####################       ####################

vcf_short <- vcf_short %>%
  dplyr::group_by(variant_id) %>% # group_by keeps all variant_id for all rows
  mutate(
    weight = {
      # Extract allele frequencies for short_living and long_living (CE vs CL)
      short_living <- mean(allele_frequency[fly_group == "short_living"], na.rm = TRUE )
      long_living <- mean(allele_frequency[fly_group == "long_living"], na.rm = TRUE)
      # Calculate weight
      allele_diff <- abs(short_living - long_living)        # Based on Absolute difference in allele frequencies
      max_frequency <- pmin(short_living, long_living)      # based on minimum average allele frequency to determine allele fixation
      weight_value <- (allele_diff) * (1 - max_frequency) # Penalized weight equation
      # Assign the single weight value to all rows
      weight_value
    }
  )

# Save the motherfucker!
write.csv(vcf_short, "/Users/hamid/Desktop/GEN MSc Project/Data/SoftWeight_vcf_short_full.csv")
# read it if you need it
vcf_short <- read.csv("/Users/hamid/Desktop/GEN MSc Project/Data/vcf_short_full.csv")


#################### Chunk ####################
# // check for statistical assumptions
# // Model selection should be based in assumptions and data statistics 
####################       ####################

# Plot histogram of allele frequencies
variant_density <- ggplot(vcf_short, aes(x = allele_frequency)) +
  geom_histogram(aes(y = ..count..), bins = 40, fill = "grey", alpha = 0.8, color = "black") +
  stat_bin(bins = 40, geom = "line", aes(y = ..count..), color = "red", size = 0.3) +
  labs(title = "Allele Frequencies Distribution", x = "Allele Frequency", y = "Count", subtitle = "Passed CRISP EM-algorithm - variant for each sample") +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 8), plot.subtitle = element_text(size = 6),
    axis.title.x = element_text(size = 8), axis.title.y = element_text(size = 8),
    axis.text.y = element_text(size = 6, color = "black"), axis.text.x = element_text(size = 6, color = "black"),
    panel.grid.major = element_line(color = scales::alpha("black", 0.3), size = 0.1),
    panel.grid.minor = element_line(color = scales::alpha("black", 0.3), size = 0.1))

print(variant_density)

# Summarize allele frequencies by biological group
group_summary <- vcf_short %>%
  group_by(biological_group) %>%
  summarize(
    mean_af = mean(allele_frequency, na.rm = TRUE),
    sd_af = sd(allele_frequency, na.rm = TRUE),
    .groups = "drop"
  )
# View the summary
print(group_summary)
print(var(vcf_short$allele_frequency))
print(sd(vcf_short$allele_frequency))

rm(variant_density)
rm(group_summary)
gc()



#################### Chunk #################### 
# // Check the roup diference in allele frequency between the CE and CL
# // We add up the CE2 adn CE3 as CE group and CL2 and CL3 as CL group
####################       ####################

# Calculate group-level differences for each variant
variant_differences <- vcf_short %>%
  group_by(CHROM, POS, REF, ALT, biological_group) %>%
  summarize(mean_af = mean(allele_frequency, na.rm = TRUE), .groups = "drop"
  ) %>%
  pivot_wider(names_from = biological_group, values_from = mean_af) %>%
  mutate(diff_CE_CL = abs((CE2 + CE3) / 2 - (CL2 + CL3) / 2)) # Average CE vs. CL



#################### Chunk #################### 
# // Visualize a variant between CE and CL
# // Visualization is based on variant location and chromosome position
# // We include visualization of multi alleleic variants
####################       ####################

# Select a variant of interest based on CHROM and POS
key_variant <- vcf_short %>%
  filter(CHROM == "X", POS == 12418979) %>% 
  mutate(allele_group = paste0(REF, " → ", ALT))  # Include REF and ALT in the group

# Plot allele frequencies for all ALT alleles
ggplot(key_variant, aes(x = biological_group, y = allele_frequency, fill = allele_group, color = biological_group)) +
  geom_boxplot(alpha = 0.8, outlier.shape = NA, position = position_dodge(width = 0.8), size = 0.5, color = "black") +  # Black borders for boxplots
  geom_jitter(alpha = 1, size = 1, position = position_dodge(width = 0.8)) +  # Subtle jitter points
  labs(
    title = paste0("Allele Frequency Differences for Variant: ", unique(key_variant$variant_id)),
    x = "Biological Group",
    y = "Allele Frequency",
    fill = "Allele (Ref → Alt)",
    color = "Biological Group"
  ) +
  scale_fill_brewer(palette = "Set2") +  # Use a scientific color palette for allele groups
  theme_minimal(base_size = 12) +  # Slightly larger font size for a scientific look
  theme(
    legend.position = "right",  # Keep the legend on the right side
    legend.box = "vertical",  # Arrange legend vertically
    legend.title = element_text(face = "bold", size = 10),  # Bold legend title
    legend.text = element_text(size = 10),  # Adjust legend text size
    axis.title = element_text(face = "bold"),  # Bold axis titles
    axis.text = element_text(size = 10),  # Slightly larger axis text
    plot.title = element_text(face = "bold", size = 12, hjust = 0.5),  # Centered and bold title
    panel.grid.major = element_line(color = "gray90", size = 0.25),  # Faint gridlines
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    panel.background = element_rect(fill = "white", color = NA)  # Clean background
  )
rm(key_variant) # remove when no longer in use
gc()



#################### Chunk #################### 
# // For validation and experimentation!!!!!!!!!!!
# // Run the GLMM with beta distribution
# // Run on a single variant of choice based on CHROM and POS
# // This chunck is used for model validation using the "variant_differences" above
####################       ####################

# ORIGINAL
variant_data <- vcf_short %>%
  dplyr::filter(CHROM == "2L" & POS == 10000718)
# fix 0 and 1s
epsilon <- .Machine$double.eps
variant_data$allele_frequency <- ifelse(
  variant_data$allele_frequency == 0, epsilon,
  ifelse(variant_data$allele_frequency == 1, 1-epsilon, variant_data$allele_frequency)
)
rm(epsilon)

# Fit the model
model_g <- glmmTMB(
  allele_frequency ~ fly_group + age_group*fly_group + (1 | biological_group) + (1 | biological_group:technical_group),
  data = variant_data, contrasts=list(age_group="contr.sum", fly_group="contr.sum"), # contrast is required for type3 Chisqaure to test for interaction term. (sum-to-zero contrasts on factors and center numerical covariates is important, as it ensures fair comparison across factor levels in models with interactions or multiple fixed effects)
  family = beta_family(link = "logit"), weights = weight # Logit equals all values between 0 and 1 (NAs allowed)
)
summary(model_g) # wald z test
Anova(model_g, type = 3) # Anova type 3

# TEST ASSUMPTION
VarCorr(model_g)
# 1. Simulate residuals
model_g <- simulateResiduals(model_g)
# 2. Plot diagnostics
plot(model_g)
# 3. Test uniformity of residuals
testUniformity(model_g)
# 4. Test dispersion (checks heteroscedasticity)
testDispersion(model_g)
# 5. Random effects diagnostics
testZeroInflation(model_g)
gc()
rm(weights)
rm(model_g)
rm(variant_data)
```
